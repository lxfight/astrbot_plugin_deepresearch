# main.py
import asyncio
import json
import httpx
import re
import markdown
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any, AsyncGenerator, Union

# å¯¼å…¥ AstrBot API
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api import logger, AstrBotConfig
from astrbot.api.provider import Provider, LLMResponse
import astrbot.api.message_components as Comp

from .search_engine_lib.models import SearchQuery, SearchResponse, SearchResultItem
from .search_engine_lib.base import BaseSearchEngine
from .search_engine_lib import initialize, list_engines, get_engine

from .core.constants import (
    PLUGIN_NAME,
    PLUGIN_VERSION,
    PLUGIN_DESCRIPTION,
    PLUGIN_AUTHOR,
    PLUGIN_REPO,
)

# HTML æŠ¥å‘Šæ¨¡æ¿ï¼Œç”¨äº html_render
HTML_REPORT_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Deep Research Report</title>
<style>
  body {{
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
    line-height: 1.6;
    color: #333;
    background-color: #f9f9f9;
    padding: 20px;
    max-width: 900px;
    margin: 20px auto;
    border: 1px solid #eee;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    border-radius: 8px;
  }}
  h1, h2, h3 {{ color: #0056b3; border-bottom: 1px solid #eee; padding-bottom: 5px;}}
  h1 {{ text-align: center; }}
  a {{ color: #007bff; text-decoration: none; }}
  a:hover {{ text-decoration: underline; }}
  pre {{ background-color: #eee; padding: 10px; border-radius: 4px; overflow-x: auto; }}
  code {{ background-color: #eee; padding: 2px 4px; border-radius: 3px; font-size: 0.9em;}}
  blockquote {{ border-left: 4px solid #ccc; padding-left: 15px; margin-left: 0; color: #555; font-style: italic;}}
   img {{ max-width: 100%; height: auto; }}
   ul, ol {{ padding-left: 25px; }}
   li {{ margin-bottom: 8px;}}
  .footer {{ margin-top: 30px; font-size: 0.8em; color: #777; text-align: center; border-top: 1px solid #eee; padding-top: 10px;}}
</style>
</head>
<body>
  <h1>æ·±åº¦ç ”ç©¶æŠ¥å‘Š</h1>
  {content}
  <div class="footer">Generated by AstrBot DeepResearch Plugin</div>
</body>
</html>
"""
# å†…å®¹æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œé˜²æ­¢LLMä¸Šä¸‹æ–‡è¿‡é•¿
MAX_CONTENT_LENGTH = 6000
# æœ€å¤šç­›é€‰é“¾æ¥æ•°
MAX_SELECTED_LINKS = 50
# æŠ“å–è¶…æ—¶
FETCH_TIMEOUT = 30.0


@register(
    PLUGIN_NAME,
    PLUGIN_AUTHOR,
    PLUGIN_DESCRIPTION,
    PLUGIN_VERSION,
    PLUGIN_REPO,
)
class DeepResearchPlugin(Star):
    """
    AstrBot æ·±åº¦ç ”ç©¶æ’ä»¶ï¼Œå®ç°æŸ¥è¯¢å¤„ç†ã€ä¿¡æ¯æ£€ç´¢ã€å†…å®¹å¤„ç†ã€æŠ¥å‘Šç”Ÿæˆå››ä¸ªé˜¶æ®µã€‚
    """

    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.context = context
        self.config = config
        # åˆå§‹åŒ–å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
        self.client = httpx.AsyncClient(
            timeout=FETCH_TIMEOUT, http2=True, follow_redirects=True, verify=False
        )
        self.search_engine_initialized = False
        self.available_engine_names: List[str] = []
        self.max_count: int = self.config.get("max_search_results_per_term", 6)
        self.max_terms: int = self.config.get("max_terms_to_search", 3)
        engine_config = self.config.get("engine_config", {})
        asyncio.create_task(self.initialize_engine(engine_config))
        logger.info("DeepResearchPlugin åˆå§‹åŒ–å®Œæˆï¼ŒHTTP å®¢æˆ·ç«¯å·²åˆ›å»ºã€‚")

    async def initialize_engine(self, engine_config):
        try:
            logger.info("DeepResearchPlugin: æ­£åœ¨ä½¿ç”¨é…ç½®åˆå§‹åŒ– search_engine_lib...")
            await initialize(engine_config)
            # è·å–æ‰€æœ‰åˆå§‹åŒ–æˆåŠŸçš„å¼•æ“åˆ—è¡¨
            self.available_engine_names = list_engines()

            # æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªå¼•æ“å¯ç”¨
            if not self.available_engine_names:
                logger.error(
                    "DeepResearchPlugin: search_engine_lib åˆå§‹åŒ–å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨/å·²é…ç½®çš„å¼•æ“ï¼è¯·æ£€æŸ¥ engine_configã€‚"
                )
                self.search_engine_initialized = False
            else:
                self.search_engine_initialized = True
                logger.info(
                    f"DeepResearchPlugin: search_engine_lib åˆå§‹åŒ–æˆåŠŸã€‚å°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨å¼•æ“: {self.available_engine_names}"
                )
        except Exception as e:
            logger.error(
                f"DeepResearchPlugin: åˆå§‹åŒ– search_engine_lib å¤±è´¥: {e}", exc_info=True
            )
            self.search_engine_initialized = False
            self.available_engine_names = []  # ç¡®ä¿å¤±è´¥æ—¶åˆ—è¡¨ä¸ºç©º

    async def terminate(self):
        """æ’ä»¶ç»ˆæ­¢æ—¶æ¸…ç†èµ„æº"""
        if hasattr(self, "client") and not self.client.is_closed:
            await self.client.aclose()
            logger.info("DeepResearchPlugin èµ„æºå·²é‡Šæ”¾ï¼ŒHTTP å®¢æˆ·ç«¯å·²å…³é—­ã€‚")

    # ------------------ LLM è°ƒç”¨è¾…åŠ©å‡½æ•° ------------------
    async def _call_llm(
        self, provider: Provider, prompt: str, system_prompt: str = ""
    ) -> Optional[str]:
        """å°è£… LLM è°ƒç”¨ï¼Œè¿”å›æ–‡æœ¬å†…å®¹æˆ– None"""
        try:
            # è°ƒç”¨ AstrBot æä¾›çš„ LLM æ¥å£
            llm_response: LLMResponse = await provider.text_chat(
                prompt=prompt,
                session_id=None,
                contexts=[],
                image_urls=[],
                func_tool=None,
                system_prompt=system_prompt,
            )
            if (
                llm_response
                and llm_response.role == "assistant"
                and llm_response.completion_text
            ):
                # å°è¯•æ¸…ç† JSON å­—ç¬¦ä¸²å‰åçš„ markdown æ ‡è®°
                content = llm_response.completion_text.strip()
                content = re.sub(r"^```json\s*", "", content, flags=re.IGNORECASE)
                content = re.sub(r"\s*```$", "", content, flags=re.IGNORECASE)
                return content
            else:
                logger.warning(f"LLM è°ƒç”¨æœªè¿”å›æœ‰æ•ˆåŠ©æ‰‹æ¶ˆæ¯: {llm_response}")
                return None
        except Exception as e:
            logger.error(f"è°ƒç”¨ LLM å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return None

    # ------------------ é˜¶æ®µä¸€ï¼šæŸ¥è¯¢å¤„ç†ä¸æ‰©å±• (Query Processing) ------------------
    async def _stage1_query_processing(
        self, provider: Provider, query: str
    ) -> Optional[Dict[str, Any]]:
        """é˜¶æ®µä¸€ï¼šä½¿ç”¨ LLM è§£æå’Œæ‰©å±•ç”¨æˆ·æŸ¥è¯¢"""
        logger.info(f"é˜¶æ®µä¸€ï¼šå¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯è§£æç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼Œå¹¶å°†å…¶åˆ†è§£å’Œæ‰©å±•ï¼Œä»¥ä¾¿è¿›è¡Œåç»­çš„ä¿¡æ¯æ£€ç´¢ã€‚
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ–‡æœ¬ã€‚
        æ ¼å¼è¦æ±‚ï¼š
        {
          "original_question": "ç”¨æˆ·è¾“å…¥çš„åŸè¯",
          "sub_questions": ["å°†å¤æ‚é—®é¢˜æ‹†è§£æˆçš„å…·ä½“ã€æ˜“äºæ£€ç´¢çš„å°é—®é¢˜åˆ—è¡¨"],
          "sub_topics": ["é—®é¢˜ä¸­åŒ…å«çš„ç›¸å…³ä¸»é¢˜å…³é”®è¯åˆ—è¡¨"],
          "expansion_questions": ["åŸºäºåŸå§‹é—®é¢˜ï¼Œç”Ÿæˆçš„æœ‰åŠ©äºæä¾›æ›´å…¨é¢ç­”æ¡ˆçš„æ‰©å±•æ€§é—®é¢˜åˆ—è¡¨"],
          "search_queries": ["ç»“åˆä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆ 3-5 ä¸ªç”¨äºæœç´¢å¼•æ“çš„é«˜è´¨é‡æœç´¢å…³é”®è¯çŸ­è¯­åˆ—è¡¨"]
        }
        """
        response_text = await self._call_llm(provider, query, system_prompt)
        if not response_text:
            return None
        try:
            parsed_data = json.loads(response_text)
            # å°†æ‰€æœ‰é—®é¢˜å’Œæœç´¢è¯åˆå¹¶ï¼Œç”¨äºåç»­æœç´¢
            all_search_terms = set()
            all_search_terms.add(query)
            all_search_terms.update(parsed_data.get("sub_questions", []))
            all_search_terms.update(parsed_data.get("sub_topics", []))
            all_search_terms.update(parsed_data.get("expansion_questions", []))
            all_search_terms.update(parsed_data.get("search_queries", []))
            parsed_data["all_search_terms"] = list(all_search_terms)
            logger.info(
                f"é˜¶æ®µä¸€ï¼šæŸ¥è¯¢è§£ææˆåŠŸã€‚ç”Ÿæˆæœç´¢è¯ {len(parsed_data['all_search_terms'])} ä¸ªã€‚"
            )
            return parsed_data
        except json.JSONDecodeError:
            logger.error(f"é˜¶æ®µä¸€ï¼šLLM è¿”å›çš„ JSON è§£æå¤±è´¥: {response_text[:200]}...")
            return None

    # ------------------ é˜¶æ®µäºŒï¼šä¿¡æ¯æ£€ç´¢ä¸ç­›é€‰ (Information Retrieval & Filtering) ------------------
    # --- æ–°å¢: å•ä¸ªæœç´¢è¯æŸ¥è¯¢è¾…åŠ©å‡½æ•° ---
    async def _run_single_search(
        self, engine: BaseSearchEngine, term: str, count: int
    ) -> List[SearchResultItem]:
        """ä½¿ç”¨æŒ‡å®šå¼•æ“å’Œæœç´¢è¯æ‰§è¡Œä¸€æ¬¡æœç´¢ï¼Œå¹¶å¤„ç†å¼‚å¸¸"""
        if not term:
            return []
        logger.info(f"ä½¿ç”¨å¼•æ“ '{engine.name}' æœç´¢: '{term}' (count={count})")
        try:
            query_obj = SearchQuery(query=term, count=count)
            response: SearchResponse = await engine.search(query_obj)
            logger.debug(f"æœç´¢ '{term}' è¿”å› {len(response.results)} æ¡ç»“æœã€‚")
            return response.results
        except Exception as e:
            logger.error(
                f"ä½¿ç”¨å¼•æ“ '{engine.name}' æœç´¢ '{term}' æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True
            )
            return []

    # ----------------------------------
    async def _search_web(self, search_terms: List[str]) -> List[Dict[str, str]]:
        """
        é˜¶æ®µäºŒï¼šå¤šæºä¿¡æ¯æ£€ç´¢
        ä½¿ç”¨ search_engine_lib ä¸­ã€æ‰€æœ‰å¯ç”¨å¼•æ“ã€‘å¹¶å‘æœç´¢å¤šä¸ªå…³é”®è¯ï¼Œå¹¶åˆå¹¶ã€å»é‡ã€æ ¼å¼åŒ–ç»“æœã€‚
        """
        # æ£€æŸ¥åˆå§‹åŒ–çŠ¶æ€å’Œå¼•æ“åˆ—è¡¨
        if not self.search_engine_initialized or not self.available_engine_names:
            logger.error(
                "é˜¶æ®µäºŒï¼šsearch_engine_lib æœªåˆå§‹åŒ–ã€ä¸å¯ç”¨ï¼Œæˆ–æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨å¼•æ“ï¼Œæ— æ³•æ‰§è¡Œæœç´¢ã€‚"
            )
            return []

        if not search_terms:
            logger.warning("é˜¶æ®µäºŒï¼šæ²¡æœ‰æä¾›æœç´¢è¯ã€‚")
            return []
        # è·å–æ‰€æœ‰å¯ç”¨çš„å¼•æ“å®ä¾‹
        engines: List[BaseSearchEngine] = []
        for name in self.available_engine_names:
            try:
                engine = get_engine(name)
                if engine:
                    engines.append(engine)
            except Exception as e:
                # å¦‚æœè·å–æŸä¸ªå¼•æ“å¤±è´¥ï¼Œè®°å½•æ—¥å¿—å¹¶è·³è¿‡ï¼Œç»§ç»­ä½¿ç”¨å…¶ä»–å¼•æ“
                logger.warning(
                    f"é˜¶æ®µäºŒï¼šè·å–å¼•æ“ '{name}' å®ä¾‹å¤±è´¥: {e}ï¼Œå°†è·³è¿‡æ­¤å¼•æ“ã€‚"
                )

        if not engines:
            logger.error("é˜¶æ®µäºŒï¼šæ— æ³•è·å–ä»»ä½•æœ‰æ•ˆçš„æœç´¢å¼•æ“å®ä¾‹ã€‚")
            return []
        # é™åˆ¶å®é™…ç”¨äºæœç´¢çš„è¯æ¡æ•°é‡
        terms_to_search = [term for term in search_terms if term][: self.max_terms]

        engine_names_str = ", ".join([e.name for e in engines])
        logger.warning(
            f"é˜¶æ®µäºŒï¼šæ³¨æ„ API æ¶ˆè€—ï¼å‡†å¤‡ä½¿ç”¨ {len(engines)} ä¸ªå¼•æ“ ({engine_names_str}) å¯¹ {len(terms_to_search)} ä¸ªè¯æ¡è¿›è¡Œå¹¶å‘æœç´¢ (æ¯ä¸ªç»„åˆæœ€å¤š {self.max_count} æ¡ç»“æœ)..."
        )
        logger.info(
            f"é˜¶æ®µäºŒï¼šæ€»è®¡å°†æ‰§è¡Œæœ€å¤š {len(engines) * len(terms_to_search)} æ¬¡æœç´¢ API è°ƒç”¨ã€‚"
        )
        # åˆ›å»ºå¹¶è¡Œæœç´¢ä»»åŠ¡åˆ—è¡¨: [engine1_term1, engine1_term2, ..., engine2_term1, engine2_term2, ...]
        tasks = []
        for engine in engines:  # éå†æ¯ä¸ªå¼•æ“
            for term in terms_to_search:  # éå†æ¯ä¸ªæœç´¢è¯
                tasks.append(self._run_single_search(engine, term, self.max_count))

        # å¹¶è¡Œæ‰§è¡Œ
        all_results_nested: List[
            Union[List[SearchResultItem], Exception]
        ] = await asyncio.gather(*tasks, return_exceptions=True)
        # å±•å¹³ç»“æœåˆ—è¡¨ï¼Œè¿‡æ»¤æ‰å¼‚å¸¸ï¼Œå¹¶è½¬æ¢æ ¼å¼ + å»é‡
        formatted_results: List[Dict[str, str]] = []
        seen_urls = set()
        total_items_found = 0

        for result_batch in all_results_nested:
            if isinstance(result_batch, list):
                total_items_found += len(result_batch)
                for item in result_batch:
                    url_str = str(item.link)
                    if url_str not in seen_urls:
                        formatted_results.append(
                            {
                                "title": item.title,
                                "url": url_str,
                                "snippet": item.snippet,
                            }
                        )
                        seen_urls.add(url_str)
            elif isinstance(result_batch, Exception):
                logger.warning(
                    f"ä¸€ä¸ªæœç´¢ä»»åŠ¡å¤±è´¥: {result_batch}"
                )  # å“ªä¸ªå¼•æ“å“ªä¸ªè¯å¤±è´¥ä¼šåœ¨ _run_single_search ä¸­è®°å½•

        logger.info(
            f"é˜¶æ®µäºŒï¼šæ‰€æœ‰æœç´¢å¼•æ“å…±æ‰¾åˆ° {total_items_found} æ¡ç»“æœï¼Œåˆå¹¶å»é‡åå‰©ä½™ {len(formatted_results)} æ¡ã€‚"
        )
        return formatted_results

    async def _stage2_link_selection(
        self, provider: Provider, original_query: str, links: List[Dict[str, str]]
    ) -> List[str]:
        """é˜¶æ®µäºŒï¼šé“¾æ¥å»é‡ä¸ LLM ç­›é€‰"""
        # _search_web å·²ç»å»é‡è¿‡ï¼Œè¿™é‡Œå¯ä»¥ç®€åŒ–æˆ–ä¿ç•™ä½œä¸ºåŒé‡ä¿é™©
        unique_links_dict = {link["url"]: link for link in links}
        unique_links = list(unique_links_dict.values())
        if not unique_links:
            return []
        logger.info(
            f"é˜¶æ®µäºŒï¼šå‡†å¤‡ä» {len(unique_links)} ä¸ªé“¾æ¥ä¸­è¿›è¡Œ LLM ç­›é€‰ï¼Œæœ€å¤šé€‰æ‹© {MAX_SELECTED_LINKS} ä¸ª..."
        )  # æ›´æ–°æ—¥å¿—
        link_descriptions = "\n".join(
            [
                f"- URL: {link['url']}\n  Title: {link['title']}\n  Snippet: {link.get('snippet', '')}"
                for link in unique_links
            ]
        )

        # æ›´æ–° prompt ä¸­çš„ MAX_SELECTED_LINKS
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»å€™é€‰é“¾æ¥åˆ—è¡¨ä¸­ï¼Œæ ¹æ®ä¸åŸå§‹é—®é¢˜çš„ç›¸å…³æ€§ï¼Œç­›é€‰å‡ºæœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„æœ€å¤š {MAX_SELECTED_LINKS} ä¸ªé“¾æ¥ã€‚
        åŸå§‹é—®é¢˜ï¼š "{original_query}"

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON åˆ—è¡¨æ ¼å¼è¿”å›ç»“æœï¼ŒåªåŒ…å«é€‰å®šé“¾æ¥çš„ URL å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ–‡æœ¬ã€‚
        æ ¼å¼è¦æ±‚ï¼š
        ["url1", "url2", "url3"]
        å¦‚æœæ²¡æœ‰ä»»ä½•é“¾æ¥ç›¸å…³ï¼Œè¿”å›ç©ºåˆ—è¡¨: []
        """
        prompt = f"è¯·ä»ä»¥ä¸‹é“¾æ¥ä¸­ç­›é€‰å‡ºæœ€ç›¸å…³çš„æœ€å¤š {MAX_SELECTED_LINKS} ä¸ªï¼š\n\n{link_descriptions}"
        response_text = await self._call_llm(provider, prompt, system_prompt)
        if not response_text:
            return []
        try:
            selected_urls = json.loads(response_text)
            if not isinstance(selected_urls, list):
                raise TypeError("LLM did not return a list")
            final_list = [
                str(url) for url in selected_urls if str(url) in unique_links_dict
            ][:MAX_SELECTED_LINKS]  # ä½¿ç”¨æ›´æ–°åçš„ MAX_SELECTED_LINKS
            logger.info(f"é˜¶æ®µäºŒï¼šLLM ç­›é€‰å®Œæˆï¼Œé€‰å®š {len(final_list)} ä¸ªé“¾æ¥ã€‚")
            return final_list
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(
                f"é˜¶æ®µäºŒï¼šLLM é“¾æ¥ç­›é€‰ç»“æœ JSON è§£æå¤±è´¥ ({e}): {response_text[:200]}..."
            )
            return list(unique_links_dict.keys())[:MAX_SELECTED_LINKS]

    # ------------------ é˜¶æ®µä¸‰ï¼šå†…å®¹å¤„ç†ä¸åˆ†æ (Content Processing & Analysis) ------------------

    async def _fetch_and_parse_content(self, url: str) -> Optional[str]:
        """æŠ“å–å•ä¸ªURLå†…å®¹å¹¶æå–æ¸…æ´—åçš„æ–‡æœ¬"""
        logger.info(f"é˜¶æ®µä¸‰ï¼šæ­£åœ¨æŠ“å– URL: {url}")
        try:
            async with self.client as client:
                response = await client.get(
                    url, headers={"User-Agent": "Mozilla/5.0 AstrBot/1.0"}
                )
                response.raise_for_status()  # æ£€æŸ¥ HTTP é”™è¯¯

                # ä½¿ç”¨ BeautifulSoup æå–æ–‡æœ¬
                soup = BeautifulSoup(response.content, "lxml")
                # ç§»é™¤ script å’Œ style æ ‡ç­¾
                for script in soup(["script", "style"]):
                    script.decompose()

                text = soup.get_text()
                # æ¸…æ´—æ–‡æœ¬ï¼šç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
                text = re.sub(r"\s+", " ", text).strip()
                # æˆªæ–­è¿‡é•¿æ–‡æœ¬
                logger.info(f"é˜¶æ®µä¸‰ï¼šURL {url} æŠ“å–å®Œæˆï¼Œæ¸…æ´—åé•¿åº¦: {len(text)}")
                return text[:MAX_CONTENT_LENGTH]
        except httpx.HTTPStatusError as e:
            logger.warning(
                f"æŠ“å– URL {url} å¤±è´¥ï¼ŒHTTP çŠ¶æ€ç : {e.response.status_code}"
            )
        except httpx.RequestError as e:
            logger.warning(f"æŠ“å– URL {url} å¤±è´¥ï¼Œè¯·æ±‚é”™è¯¯: {e}")
        except Exception as e:
            logger.error(f"æŠ“å–æˆ–è§£æ URL {url} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        return None

    async def _summarize_content(
        self, provider: Provider, query: str, url: str, content: str
    ) -> Optional[str]:
        """ä½¿ç”¨ LLM æ€»ç»“å•ä¸ªæ–‡æ¡£å†…å®¹"""
        logger.info(f"é˜¶æ®µä¸‰ï¼šæ­£åœ¨æ€»ç»“ URL {url} çš„å†…å®¹...")
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œæ€»ç»“å‡ºä¸åŸå§‹æŸ¥è¯¢ï¼šâ€œ{query}â€ é«˜åº¦ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚
        æ€»ç»“åº”æ¸…æ™°ã€ç®€æ´ï¼Œçªå‡ºè¦ç‚¹ã€‚å¿½ç•¥å¹¿å‘Šã€å¯¼èˆªç­‰æ— å…³å†…å®¹ã€‚
        è¯·ç›´æ¥è¿”å›æ€»ç»“æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€æ ‡é¢˜æˆ–é—®å€™è¯­ã€‚
        """
        prompt = f"è¯·æ ¹æ®æŸ¥è¯¢ â€œ{query}â€ æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼š\n\n---\n{content}\n---"
        summary = await self._call_llm(provider, prompt, system_prompt)
        if summary:
            logger.info(f"é˜¶æ®µä¸‰ï¼šURL {url} æ€»ç»“å®Œæˆã€‚")
        else:
            logger.warning(f"é˜¶æ®µä¸‰ï¼šURL {url} æ€»ç»“å¤±è´¥ã€‚")
        return summary

    async def _process_one_link(
        self, provider: Provider, query: str, url: str
    ) -> Optional[Dict[str, str]]:
        """å¤„ç†å•ä¸ªé“¾æ¥ï¼šæŠ“å– -> æ€»ç»“"""
        content = await self._fetch_and_parse_content(url)
        if content and len(content) > 100:  # å¿½ç•¥å†…å®¹è¿‡å°‘çš„é¡µé¢
            summary = await self._summarize_content(provider, query, url, content)
            if summary:
                return {"url": url, "summary": summary}
        return None

    async def _stage3_content_processing(
        self, provider: Provider, query: str, selected_links: List[str]
    ) -> List[Dict[str, str]]:
        """é˜¶æ®µä¸‰ï¼šå¹¶è¡ŒæŠ“å–å†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦"""
        logger.info("é˜¶æ®µä¸‰ï¼šå¼€å§‹å¹¶è¡ŒæŠ“å–å’Œæ€»ç»“å†…å®¹...")
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        tasks = [self._process_one_link(provider, query, url) for url in selected_links]
        # å¹¶è¡Œæ‰§è¡Œï¼Œå…è®¸éƒ¨åˆ†å¤±è´¥
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # è¿‡æ»¤æ‰å¤±è´¥æˆ–æ— æ•ˆçš„ç»“æœ
        summaries = [
            res
            for res in results
            if isinstance(res, dict) and res is not None and "summary" in res
        ]
        logger.info(
            f"é˜¶æ®µä¸‰ï¼šæˆåŠŸå¤„ç†å¹¶æ€»ç»“äº† {len(summaries)} / {len(selected_links)} ä¸ªé“¾æ¥ã€‚"
        )
        return summaries

    async def _stage3_aggregation(
        self,
        provider: Provider,
        query: str,
        expansion_questions: List[str],
        summaries: List[Dict[str, str]],
    ) -> Optional[str]:
        """é˜¶æ®µä¸‰ï¼šLLM èšåˆåˆ†ææ‰€æœ‰æ‘˜è¦ï¼Œç”Ÿæˆ Markdown æŠ¥å‘Š"""
        logger.info("é˜¶æ®µä¸‰ï¼šå¼€å§‹èšåˆåˆ†ææ‰€æœ‰æ‘˜è¦...")
        if not summaries:
            return "æœªèƒ½ä»ä»»ä½•æ¥æºè·å–æœ‰æ•ˆæ‘˜è¦ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚"
        # å‡†å¤‡ LLM è¾“å…¥
        summaries_input = "\n\n".join(
            [f"### æ¥æº: {item['url']}\n{item['summary']}\n---" for item in summaries]
        )
        expansion_q_str = (
            "\n".join([f"- {q}" for q in expansion_questions])
            if expansion_questions
            else "æ— "
        )
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªé«˜çº§ç ”ç©¶åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ç»¼åˆæ¥è‡ªå¤šä¸ªæ¥æºçš„æ‘˜è¦ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°ã€å†…å®¹è¿è´¯ã€é€»è¾‘ä¸¥å¯†çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚

        åŸå§‹æŸ¥è¯¢: "{query}"

        éœ€è¦é¢å¤–è€ƒè™‘å’Œå›ç­”çš„æ‰©å±•é—®é¢˜:
        {expansion_q_str}
        æŠ¥å‘Šè¦æ±‚ï¼š
        1. æ ¼å¼ï¼šä½¿ç”¨æ ‡å‡†çš„ Markdown è¯­æ³•ã€‚
        2. ç»“æ„ï¼šåº”åŒ…å«æ ‡é¢˜ã€å¼•è¨€ã€ä¸»ä½“æ®µè½ï¼ˆå¯ä»¥æŒ‰ä¸»é¢˜æˆ–æ‰©å±•é—®é¢˜åˆ†èŠ‚ï¼‰ã€ç»“è®ºã€‚
        3. å†…å®¹ï¼šç»¼åˆæ‰€æœ‰æ¥æºçš„ä¿¡æ¯ï¼Œå¯¹æ¯”ä¸åŒè§‚ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œæ•´åˆä¿¡æ¯ï¼Œæ„å»ºé€»è¾‘ã€‚
        4. å¼•ç”¨ï¼šåœ¨å¼•ç”¨äº†æŸä¸ªæ¥æºä¿¡æ¯çš„å¥å­æˆ–æ®µè½æœ«å°¾ï¼Œæ˜ç¡®æ ‡æ³¨æ¥æºï¼Œæ ¼å¼ä¸º ` [æ¥æº: URL]`ã€‚
        5. ç›®æ ‡ï¼šå…¨é¢ã€æ·±å…¥åœ°å›ç­”åŸå§‹æŸ¥è¯¢åŠæ‰©å±•é—®é¢˜ã€‚
        6. è¾“å‡ºï¼šç›´æ¥è¾“å‡º Markdown æŠ¥å‘Šæ­£æ–‡ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–é—®å€™è¯­ã€‚
        """
        prompt = f"è¯·æ ¹æ®ä»¥ä¸‹æ¥è‡ªä¸åŒæ¥æºçš„æ‘˜è¦ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½å…³äº â€œ{query}â€ çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼š\n\n{summaries_input}"
        report_markdown = await self._call_llm(provider, prompt, system_prompt)
        if report_markdown:
            logger.info("é˜¶æ®µä¸‰ï¼šèšåˆåˆ†æå®Œæˆï¼ŒMarkdown æŠ¥å‘Šå·²ç”Ÿæˆã€‚")
        else:
            logger.warning("é˜¶æ®µä¸‰ï¼šèšåˆåˆ†æå¤±è´¥ã€‚")
        return report_markdown

    # ------------------ é˜¶æ®µå››ï¼šæŠ¥å‘Šç”Ÿæˆä¸äº¤ä»˜ (Report Generation & Delivery) ------------------
    async def _stage4_report_generation(self, markdown_text: str) -> Optional[str]:
        """é˜¶æ®µå››ï¼šå°† Markdown æŠ¥å‘Šæ¸²æŸ“ä¸ºå›¾ç‰‡ URL"""
        logger.info("é˜¶æ®µå››ï¼šå¼€å§‹å°† Markdown æ¸²æŸ“ä¸ºå›¾ç‰‡...")
        try:
            # 1. Markdown è½¬ HTML
            html_body = markdown.markdown(
                markdown_text, extensions=["extra", "codehilite", "tables", "toc"]
            )
            # 2. å¡«å……æ¨¡æ¿
            full_html = HTML_REPORT_TEMPLATE.format(content=html_body)
            # 3. ä½¿ç”¨ AstrBot çš„ html_render æ¸²æŸ“å›¾ç‰‡
            # html_render æ˜¯ Star åŸºç±»çš„æ–¹æ³•
            image_url = await self.html_render(full_html, {}, return_url=True)
            logger.info("é˜¶æ®µå››ï¼šå›¾ç‰‡æŠ¥å‘Šæ¸²æŸ“æˆåŠŸã€‚")
            return image_url
        except Exception as e:
            logger.error(f"é˜¶æ®µå››ï¼šMarkdown è½¬ HTML æˆ–æ¸²æŸ“å›¾ç‰‡å¤±è´¥: {e}", exc_info=True)
            return None

    # ------------------ ä¸»æµç¨‹æ§åˆ¶ ------------------

    async def _run_research_pipeline(
        self, event: AstrMessageEvent, query: str
    ) -> AsyncGenerator[MessageEventResult, None]:
        """æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶æµç¨‹ç®¡çº¿ï¼Œä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨å‘é€ä¸­é—´çŠ¶æ€å’Œæœ€ç»ˆç»“æœ"""
        # æ£€æŸ¥ LLM
        provider = self.context.get_using_provider()
        if not provider:
            yield event.plain_result(
                "âŒ é”™è¯¯ï¼šæœªé…ç½®æˆ–å¯ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œæ— æ³•æ‰§è¡Œç ”ç©¶ã€‚"
            )
            return

        start_time = asyncio.get_running_loop().time()
        yield event.plain_result(
            f"ğŸ” æ”¶åˆ°ç ”ç©¶è¯·æ±‚: '{query}'\nâ³ å¼€å§‹é˜¶æ®µä¸€ï¼šæŸ¥è¯¢å¤„ç†ä¸æ‰©å±•..."
        )

        try:
            # é˜¶æ®µä¸€
            parsed_query = await self._stage1_query_processing(provider, query)
            if not parsed_query or not parsed_query.get("all_search_terms"):
                yield event.plain_result("âŒ é˜¶æ®µä¸€å¤±è´¥ï¼šLLMæœªèƒ½æœ‰æ•ˆè§£ææŸ¥è¯¢ã€‚")
                return
            yield event.plain_result(
                "âœ… é˜¶æ®µä¸€å®Œæˆã€‚\nâ³ å¼€å§‹é˜¶æ®µäºŒï¼šä¿¡æ¯æ£€ç´¢ä¸ç­›é€‰..."
            )
            # é˜¶æ®µäºŒ
            search_terms = parsed_query.get("search_queries", []) or parsed_query.get(
                "all_search_terms", []
            )
            initial_links = await self._search_web(search_terms)
            if not initial_links:
                yield event.plain_result(
                    "âš ï¸ é˜¶æ®µäºŒè­¦å‘Šï¼šç½‘ç»œæœç´¢æœªè¿”å›ä»»ä½•åˆå§‹ç»“æœï¼ˆæˆ–æœç´¢åŠŸèƒ½æœªå®ç°ï¼‰ã€‚"
                )
                # å¦‚æœæœç´¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®©LLMå›ç­”
                yield event.plain_result("âš ï¸ å°è¯•è®©LLMæ ¹æ®è‡ªèº«çŸ¥è¯†ç›´æ¥ç”ŸæˆæŠ¥å‘Š...")
                direct_summary = await self._summarize_content(
                    provider,
                    query,
                    "LLM Knowledge Base",
                    "è¯·åŸºäºä½ è‡ªèº«çš„çŸ¥è¯†åº“ï¼Œç”Ÿæˆä¸€ä»½å…³äºæ­¤ä¸»é¢˜çš„æŠ¥å‘Šã€‚",
                )
                if direct_summary:
                    summaries = [
                        {"url": "LLM Knowledge Base", "summary": direct_summary}
                    ]
                    selected_links = ["LLM Knowledge Base"]
                else:
                    yield event.plain_result(
                        "âŒ é˜¶æ®µäºŒå¤±è´¥ï¼šæœç´¢å’ŒLLMè‡ªèº«çŸ¥è¯†å‡æ— æ³•æä¾›ä¿¡æ¯ã€‚"
                    )
                    return
            else:
                yield event.plain_result(
                    f"â„¹ï¸ æœç´¢åˆ° {len(initial_links)} ä¸ªåˆå§‹é“¾æ¥ï¼Œå¼€å§‹ç­›é€‰..."
                )
                selected_links = await self._stage2_link_selection(
                    provider, query, initial_links
                )
                if not selected_links:
                    yield event.plain_result(
                        "âŒ é˜¶æ®µäºŒå¤±è´¥ï¼šLLMæœªèƒ½ä»ç»“æœä¸­ç­›é€‰å‡ºç›¸å…³é“¾æ¥ã€‚"
                    )
                    return
                yield event.plain_result(
                    f"âœ… é˜¶æ®µäºŒå®Œæˆã€‚ç­›é€‰å‡º {len(selected_links)} ä¸ªé“¾æ¥ã€‚\nâ³ å¼€å§‹é˜¶æ®µä¸‰ï¼šå†…å®¹å¤„ç†ä¸åˆ†æ..."
                )
                # é˜¶æ®µä¸‰ - å¤„ç†
                summaries = await self._stage3_content_processing(
                    provider, query, selected_links
                )
                if not summaries:
                    yield event.plain_result(
                        "âŒ é˜¶æ®µä¸‰å¤±è´¥ï¼šæœªèƒ½ä»ä»»ä½•é€‰å®šé“¾æ¥æŠ“å–æˆ–æ€»ç»“æœ‰æ•ˆå†…å®¹ã€‚"
                    )
                    return
                yield event.plain_result(
                    f"â„¹ï¸ å·²æŠ“å–å¹¶æ€»ç»“ {len(summaries)} ç¯‡å†…å®¹ã€‚å¼€å§‹èšåˆåˆ†æ..."
                )

            # é˜¶æ®µä¸‰ - èšåˆ
            aggregated_markdown = await self._stage3_aggregation(
                provider, query, parsed_query.get("expansion_questions", []), summaries
            )
            if not aggregated_markdown:
                yield event.plain_result("âŒ é˜¶æ®µä¸‰å¤±è´¥ï¼šLLMå†…å®¹èšåˆåˆ†æå¤±è´¥ã€‚")
                return
            yield event.plain_result(
                "âœ… é˜¶æ®µä¸‰å®Œæˆã€‚\nâ³ å¼€å§‹é˜¶æ®µå››ï¼šæŠ¥å‘Šç”Ÿæˆä¸æ¸²æŸ“..."
            )
            # é˜¶æ®µå››
            report_image_url = await self._stage4_report_generation(aggregated_markdown)

            end_time = asyncio.get_running_loop().time()
            duration = round(end_time - start_time, 2)
            # æœ€ç»ˆè¾“å‡º
            status_msg = f"âœ… æ·±åº¦ç ”ç©¶å®Œæˆï¼æ€»è€—æ—¶: {duration} ç§’ã€‚"
            if report_image_url:
                # ä½¿ç”¨æ¶ˆæ¯é“¾å‘é€æ–‡æœ¬å’Œå›¾ç‰‡
                yield event.chain_result(
                    [
                        Comp.Plain(text=status_msg + "\nä¸ºæ‚¨ç”Ÿæˆäº†å›¾ç‰‡æŠ¥å‘Šï¼š"),
                        Comp.Image.fromURL(report_image_url),
                        # Comp.Plain(text="\n\n--- Markdown åŸæ–‡ ---\n" + aggregated_markdown) # å¯é€‰ï¼šé™„å¸¦MarkdownåŸæ–‡
                    ]
                )
            else:
                # å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬ Markdown
                yield event.plain_result(
                    status_msg
                    + "\nâš ï¸ å›¾ç‰‡æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä»¥ä¸‹ä¸º Markdown æ–‡æœ¬æŠ¥å‘Šï¼š\n---\n"
                    + aggregated_markdown
                )
        except asyncio.TimeoutError:
            yield event.plain_result("âŒ ç ”ç©¶è¿‡ç¨‹è¶…æ—¶ã€‚")
            logger.error("Pipeline Timeout", exc_info=True)
        except Exception as e:
            yield event.plain_result(
                f"âŒ ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {type(e).__name__} - {e}"
            )
            logger.error(f"Pipeline error for query '{query}': {e}", exc_info=True)

    @filter.command("deepresearch", alias={"ç ”ç©¶", "æ·±åº¦ç ”ç©¶"})
    async def handle_research_command(self, event: AstrMessageEvent, query: str = ""):
        """
        æŒ‡ä»¤: /deepresearch <æŸ¥è¯¢å†…å®¹>
        å¯¹æŒ‡å®šå†…å®¹è¿›è¡Œå¤šé˜¶æ®µæ·±åº¦ç ”ç©¶å¹¶ç”ŸæˆæŠ¥å‘Šã€‚
        """
        if not query:
            yield event.plain_result(
                "è¯·è¾“å…¥è¦ç ”ç©¶çš„å†…å®¹ã€‚ä¾‹å¦‚: /deepresearch äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿"
            )
            return

        # ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨æ¨¡å¼ï¼Œé€ä¸ª yield æ¶ˆæ¯
        async for message_result in self._run_research_pipeline(event, query):
            yield message_result
        event.stop_event()  # åœæ­¢äº‹ä»¶ä¼ æ’­ï¼Œé˜²æ­¢LLMå†æ¬¡é»˜è®¤å›å¤
