# main.py
import asyncio
import json
import httpx
import re
import markdown
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any, AsyncGenerator, Union

# å¯¼å…¥ AstrBot API
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api import logger, AstrBotConfig
from astrbot.api.provider import Provider, LLMResponse
import astrbot.api.message_components as Comp

# å¯¼å…¥æ–°çš„æ¨¡å—æ¶æ„
from .config import (
    DEFAULT_CONFIG,
    SUPPORTED_OUTPUT_FORMATS,
    DEFAULT_HEADERS,
    HTML_REPORT_TEMPLATE,
)
from .search_engine_lib.models import SearchQuery, SearchResponse, SearchResultItem
from .search_engine_lib.base import BaseSearchEngine
from .search_engine_lib import initialize, list_engines, get_engine
from .url_resolver import URLResolverManager
from .output_format import OutputFormatManager

from .core.constants import (
    PLUGIN_NAME,
    PLUGIN_VERSION,
    PLUGIN_DESCRIPTION,
    PLUGIN_AUTHOR,
    PLUGIN_REPO,
)

# ä»é…ç½®ä¸­è·å–å¸¸é‡
MAX_CONTENT_LENGTH = DEFAULT_CONFIG["max_content_length"]
MAX_SELECTED_LINKS = DEFAULT_CONFIG["max_selected_links"]
FETCH_TIMEOUT = DEFAULT_CONFIG["fetch_timeout"]
HEADERS = DEFAULT_HEADERS


@register(
    PLUGIN_NAME,
    PLUGIN_AUTHOR,
    PLUGIN_DESCRIPTION,
    PLUGIN_VERSION,
    PLUGIN_REPO,
)
class DeepResearchPlugin(Star):
    """
    AstrBot æ·±åº¦ç ”ç©¶æ’ä»¶ï¼Œå®ç°æŸ¥è¯¢å¤„ç†ã€ä¿¡æ¯æ£€ç´¢ã€å†…å®¹å¤„ç†ã€æŠ¥å‘Šç”Ÿæˆå››ä¸ªé˜¶æ®µã€‚
    """

    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.context = context
        self.config = config
        # åˆå§‹åŒ–å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
        self.client = httpx.AsyncClient(
            timeout=FETCH_TIMEOUT,
            http2=True,
            follow_redirects=True,
            verify=False,
            headers=HEADERS,
        )
        self.search_engine_initialized = False
        self.available_engine_names: List[str] = []
        self.max_count: int = self.config.get("max_search_results_per_term", 6)
        self.max_terms: int = self.config.get("max_terms_to_search", 3)
        engine_config = self.config.get("engine_config", {})

        # åˆå§‹åŒ–è¾“å‡ºæ ¼å¼ç®¡ç†å™¨
        output_config = {
            "default_format": self.config.get("default_output_format", "image")
        }
        self.output_manager = OutputFormatManager(output_config)

        asyncio.create_task(self.initialize_engine(engine_config))
        logger.info("DeepResearchPlugin åˆå§‹åŒ–å®Œæˆï¼ŒHTTP å®¢æˆ·ç«¯å·²åˆ›å»ºã€‚")

    async def initialize_engine(self, engine_config):
        try:
            logger.info("DeepResearchPlugin: æ­£åœ¨ä½¿ç”¨é…ç½®åˆå§‹åŒ– search_engine_lib...")
            await initialize(engine_config)
            # è·å–æ‰€æœ‰åˆå§‹åŒ–æˆåŠŸçš„å¼•æ“åˆ—è¡¨
            self.available_engine_names = list_engines()

            # æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªå¼•æ“å¯ç”¨
            if not self.available_engine_names:
                logger.error(
                    "DeepResearchPlugin: search_engine_lib åˆå§‹åŒ–å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨/å·²é…ç½®çš„å¼•æ“ï¼è¯·æ£€æŸ¥ engine_configã€‚"
                )
                self.search_engine_initialized = False
            else:
                self.search_engine_initialized = True
                logger.info(
                    f"DeepResearchPlugin: search_engine_lib åˆå§‹åŒ–æˆåŠŸã€‚å°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨å¼•æ“: {self.available_engine_names}"
                )
        except Exception as e:
            logger.error(
                f"DeepResearchPlugin: åˆå§‹åŒ– search_engine_lib å¤±è´¥: {e}", exc_info=True
            )
            self.search_engine_initialized = False
            self.available_engine_names = []  # ç¡®ä¿å¤±è´¥æ—¶åˆ—è¡¨ä¸ºç©º

    async def terminate(self):
        """
        æ’ä»¶ç»ˆæ­¢ï¼Œæ¸…ç†èµ„æº
        """
        logger.info("DeepResearchPlugin æ­£åœ¨å…³é—­ HTTP Client...")
        # å¿…é¡»æ˜¾å¼å…³é—­é•¿æœŸå­˜åœ¨çš„ client å®ä¾‹
        if hasattr(self, "client") and self.client and not self.client.is_closed:
            try:
                await self.client.aclose()
                logger.info("DeepResearchPlugin HTTP Client å·²å…³é—­ã€‚")
            except Exception as e:
                logger.error(f"DeepResearchPlugin å…³é—­ HTTP Client æ—¶å‡ºé”™: {e}")

    # ------------------ LLM è°ƒç”¨è¾…åŠ©å‡½æ•° ------------------
    async def _call_llm(
        self,
        provider: Provider,
        prompt: str,
        system_prompt: str = "",
        max_retries: int = 3,
    ) -> Optional[str]:
        """å°è£… LLM è°ƒç”¨ï¼Œå¸¦é‡è¯•å’Œé€Ÿç‡é™åˆ¶ï¼Œè¿”å›æ–‡æœ¬å†…å®¹æˆ– None"""
        for attempt in range(max_retries):
            try:
                # è°ƒç”¨ AstrBot æä¾›çš„ LLM æ¥å£
                llm_response: LLMResponse = await provider.text_chat(
                    prompt=prompt,
                    session_id=None,
                    contexts=[],
                    image_urls=[],
                    func_tool=None,
                    system_prompt=system_prompt,
                )
                if (
                    llm_response
                    and llm_response.role == "assistant"
                    and llm_response.completion_text
                ):
                    # å°è¯•æ¸…ç† JSON å­—ç¬¦ä¸²å‰åçš„ markdown æ ‡è®°
                    content = llm_response.completion_text.strip()
                    content = re.sub(r"^```json\s*", "", content, flags=re.IGNORECASE)
                    content = re.sub(r"\s*```$", "", content, flags=re.IGNORECASE)
                    return content
                else:
                    logger.warning(f"LLM è°ƒç”¨æœªè¿”å›æœ‰æ•ˆåŠ©æ‰‹æ¶ˆæ¯: {llm_response}")
                    return None

            except Exception as e:
                error_msg = str(e).lower()

                # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
                if "rate" in error_msg or "429" in error_msg or "quota" in error_msg:
                    if attempt < max_retries - 1:
                        # æŒ‡æ•°é€€é¿å»¶è¿Ÿ
                        delay = (2**attempt) * 15  # 15ç§’, 30ç§’, 60ç§’
                        logger.warning(
                            f"LLM APIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {delay} ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})"
                        )
                        await asyncio.sleep(delay)
                        continue
                    else:
                        logger.error(f"LLM APIé€Ÿç‡é™åˆ¶ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        return None
                else:
                    logger.error(f"è°ƒç”¨ LLM å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                    return None

        return None

    # ------------------ é˜¶æ®µä¸€ï¼šæŸ¥è¯¢å¤„ç†ä¸æ‰©å±• (Query Processing) ------------------
    async def _stage1_query_processing(
        self, provider: Provider, query: str
    ) -> Optional[Dict[str, Any]]:
        """é˜¶æ®µä¸€ï¼šä½¿ç”¨ LLM è§£æå’Œæ‰©å±•ç”¨æˆ·æŸ¥è¯¢"""
        logger.info(f"é˜¶æ®µä¸€ï¼šå¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯è§£æç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼Œå¹¶å°†å…¶åˆ†è§£å’Œæ‰©å±•ï¼Œä»¥ä¾¿è¿›è¡Œåç»­çš„ä¿¡æ¯æ£€ç´¢ã€‚
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ–‡æœ¬ã€‚
        æ ¼å¼è¦æ±‚ï¼š
        {
            "original_question": "ç”¨æˆ·è¾“å…¥çš„åŸè¯",
            "sub_questions": ["å°†å¤æ‚é—®é¢˜æ‹†è§£æˆçš„å…·ä½“ã€æ˜“äºæ£€ç´¢çš„å°é—®é¢˜åˆ—è¡¨"],
            "sub_topics": ["é—®é¢˜ä¸­åŒ…å«çš„ç›¸å…³ä¸»é¢˜å…³é”®è¯åˆ—è¡¨"],
            "expansion_questions": ["åŸºäºåŸå§‹é—®é¢˜ï¼Œç”Ÿæˆçš„æœ‰åŠ©äºæä¾›æ›´å…¨é¢ç­”æ¡ˆçš„æ‰©å±•æ€§é—®é¢˜åˆ—è¡¨"],
            "search_queries": ["ç»“åˆä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆ 3-5 ä¸ªç”¨äºæœç´¢å¼•æ“çš„é«˜è´¨é‡æœç´¢å…³é”®è¯çŸ­è¯­åˆ—è¡¨"]
        }
        """
        response_text = await self._call_llm(provider, query, system_prompt)
        if not response_text:
            return None
        try:
            parsed_data = json.loads(response_text)
            # å°†æ‰€æœ‰é—®é¢˜å’Œæœç´¢è¯åˆå¹¶ï¼Œç”¨äºåç»­æœç´¢
            all_search_terms = set()
            all_search_terms.add(query)
            all_search_terms.update(parsed_data.get("sub_questions", []))
            all_search_terms.update(parsed_data.get("sub_topics", []))
            all_search_terms.update(parsed_data.get("expansion_questions", []))
            all_search_terms.update(parsed_data.get("search_queries", []))
            parsed_data["all_search_terms"] = list(all_search_terms)
            logger.info(
                f"é˜¶æ®µä¸€ï¼šæŸ¥è¯¢è§£ææˆåŠŸã€‚ç”Ÿæˆæœç´¢è¯ {len(parsed_data['all_search_terms'])} ä¸ªã€‚"
            )
            return parsed_data
        except json.JSONDecodeError:
            logger.error(f"é˜¶æ®µä¸€ï¼šLLM è¿”å›çš„ JSON è§£æå¤±è´¥: {response_text[:200]}...")
            return None

    # ------------------ é˜¶æ®µäºŒï¼šä¿¡æ¯æ£€ç´¢ä¸ç­›é€‰ (Information Retrieval & Filtering) ------------------
    # --- æ–°å¢: å•ä¸ªæœç´¢è¯æŸ¥è¯¢è¾…åŠ©å‡½æ•° ---
    async def _run_single_search(
        self, engine: BaseSearchEngine, term: str, count: int
    ) -> List[SearchResultItem]:
        """ä½¿ç”¨æŒ‡å®šå¼•æ“å’Œæœç´¢è¯æ‰§è¡Œä¸€æ¬¡æœç´¢ï¼Œå¹¶å¤„ç†å¼‚å¸¸"""
        if not term:
            return []
        logger.info(f"ä½¿ç”¨å¼•æ“ '{engine.name}' æœç´¢: '{term}' (count={count})")
        try:
            query_obj = SearchQuery(query=term, count=count)
            response: SearchResponse = await engine.search(query_obj)
            logger.debug(f"æœç´¢ '{term}' è¿”å› {len(response.results)} æ¡ç»“æœã€‚")
            return response.results
        except Exception as e:
            logger.error(
                f"ä½¿ç”¨å¼•æ“ '{engine.name}' æœç´¢ '{term}' æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True
            )
            return []

    # ----------------------------------
    async def _search_web(self, search_terms: List[str]) -> List[Dict[str, str]]:
        """
        é˜¶æ®µäºŒï¼šå¤šæºä¿¡æ¯æ£€ç´¢
        ä½¿ç”¨ search_engine_lib ä¸­ã€æ‰€æœ‰å¯ç”¨å¼•æ“ã€‘å¹¶å‘æœç´¢å¤šä¸ªå…³é”®è¯ï¼Œå¹¶åˆå¹¶ã€å»é‡ã€æ ¼å¼åŒ–ç»“æœã€‚
        """
        # æ£€æŸ¥åˆå§‹åŒ–çŠ¶æ€å’Œå¼•æ“åˆ—è¡¨
        if not self.search_engine_initialized or not self.available_engine_names:
            logger.error(
                "é˜¶æ®µäºŒï¼šsearch_engine_lib æœªåˆå§‹åŒ–ã€ä¸å¯ç”¨ï¼Œæˆ–æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨å¼•æ“ï¼Œæ— æ³•æ‰§è¡Œæœç´¢ã€‚"
            )
            return []

        if not search_terms:
            logger.warning("é˜¶æ®µäºŒï¼šæ²¡æœ‰æä¾›æœç´¢è¯ã€‚")
            return []
        # è·å–æ‰€æœ‰å¯ç”¨çš„å¼•æ“å®ä¾‹
        engines: List[BaseSearchEngine] = []
        for name in self.available_engine_names:
            try:
                engine = get_engine(name)
                if engine:
                    engines.append(engine)
            except Exception as e:
                # å¦‚æœè·å–æŸä¸ªå¼•æ“å¤±è´¥ï¼Œè®°å½•æ—¥å¿—å¹¶è·³è¿‡ï¼Œç»§ç»­ä½¿ç”¨å…¶ä»–å¼•æ“
                logger.warning(
                    f"é˜¶æ®µäºŒï¼šè·å–å¼•æ“ '{name}' å®ä¾‹å¤±è´¥: {e}ï¼Œå°†è·³è¿‡æ­¤å¼•æ“ã€‚"
                )

        if not engines:
            logger.error("é˜¶æ®µäºŒï¼šæ— æ³•è·å–ä»»ä½•æœ‰æ•ˆçš„æœç´¢å¼•æ“å®ä¾‹ã€‚")
            return []
        # é™åˆ¶å®é™…ç”¨äºæœç´¢çš„è¯æ¡æ•°é‡
        terms_to_search = [term for term in search_terms if term][: self.max_terms]

        engine_names_str = ", ".join([e.name for e in engines])
        logger.warning(
            f"é˜¶æ®µäºŒï¼šæ³¨æ„ API æ¶ˆè€—ï¼å‡†å¤‡ä½¿ç”¨ {len(engines)} ä¸ªå¼•æ“ ({engine_names_str}) å¯¹ {len(terms_to_search)} ä¸ªè¯æ¡è¿›è¡Œå¹¶å‘æœç´¢ (æ¯ä¸ªç»„åˆæœ€å¤š {self.max_count} æ¡ç»“æœ)..."
        )
        logger.info(
            f"é˜¶æ®µäºŒï¼šæ€»è®¡å°†æ‰§è¡Œæœ€å¤š {len(engines) * len(terms_to_search)} æ¬¡æœç´¢ API è°ƒç”¨ã€‚"
        )
        # åˆ›å»ºå¹¶è¡Œæœç´¢ä»»åŠ¡åˆ—è¡¨: [engine1_term1, engine1_term2, ..., engine2_term1, engine2_term2, ...]
        tasks = []
        for engine in engines:  # éå†æ¯ä¸ªå¼•æ“
            for term in terms_to_search:  # éå†æ¯ä¸ªæœç´¢è¯
                tasks.append(self._run_single_search(engine, term, self.max_count))

        # å¹¶è¡Œæ‰§è¡Œ
        all_results_nested: List[
            Union[List[SearchResultItem], Exception]
        ] = await asyncio.gather(*tasks, return_exceptions=True)
        # å±•å¹³ç»“æœåˆ—è¡¨ï¼Œè¿‡æ»¤æ‰å¼‚å¸¸ï¼Œå¹¶è½¬æ¢æ ¼å¼ + å»é‡
        formatted_results: List[Dict[str, str]] = []
        seen_urls = set()
        total_items_found = 0

        for result_batch in all_results_nested:
            if isinstance(result_batch, list):
                total_items_found += len(result_batch)
                for item in result_batch:
                    url_str = str(item.link)
                    if url_str not in seen_urls:
                        formatted_results.append(
                            {
                                "title": item.title,
                                "url": url_str,
                                "snippet": item.snippet,
                            }
                        )
                        seen_urls.add(url_str)
            elif isinstance(result_batch, Exception):
                logger.warning(
                    f"ä¸€ä¸ªæœç´¢ä»»åŠ¡å¤±è´¥: {result_batch}"
                )  # å“ªä¸ªå¼•æ“å“ªä¸ªè¯å¤±è´¥ä¼šåœ¨ _run_single_search ä¸­è®°å½•

        logger.info(
            f"é˜¶æ®µäºŒï¼šæ‰€æœ‰æœç´¢å¼•æ“å…±æ‰¾åˆ° {total_items_found} æ¡ç»“æœï¼Œåˆå¹¶å»é‡åå‰©ä½™ {len(formatted_results)} æ¡ã€‚"
        )
        return formatted_results

    async def _stage2_link_selection(
        self, provider: Provider, original_query: str, links: List[Dict[str, str]]
    ) -> List[str]:
        """é˜¶æ®µäºŒï¼šé“¾æ¥å»é‡ä¸ LLM ç­›é€‰"""
        # _search_web å·²ç»å»é‡è¿‡ï¼Œè¿™é‡Œå¯ä»¥ç®€åŒ–æˆ–ä¿ç•™ä½œä¸ºåŒé‡ä¿é™©
        unique_links_dict = {link["url"]: link for link in links}
        unique_links = list(unique_links_dict.values())
        if not unique_links:
            return []
        logger.info(
            f"é˜¶æ®µäºŒï¼šå‡†å¤‡ä» {len(unique_links)} ä¸ªé“¾æ¥ä¸­è¿›è¡Œ LLM ç­›é€‰ï¼Œæœ€å¤šé€‰æ‹© {MAX_SELECTED_LINKS} ä¸ª..."
        )  # æ›´æ–°æ—¥å¿—
        link_descriptions = "\n".join(
            [
                f"- URL: {link['url']}\n  Title: {link['title']}\n  Snippet: {link.get('snippet', '')}"
                for link in unique_links
            ]
        )

        # æ›´æ–° prompt ä¸­çš„ MAX_SELECTED_LINKS
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»å€™é€‰é“¾æ¥åˆ—è¡¨ä¸­ï¼Œæ ¹æ®ä¸åŸå§‹é—®é¢˜çš„ç›¸å…³æ€§ï¼Œç­›é€‰å‡ºæœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„æœ€å¤š {MAX_SELECTED_LINKS} ä¸ªé“¾æ¥ã€‚
        åŸå§‹é—®é¢˜ï¼š "{original_query}"

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON åˆ—è¡¨æ ¼å¼è¿”å›ç»“æœï¼ŒåªåŒ…å«é€‰å®šé“¾æ¥çš„ URL å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ–‡æœ¬ã€‚
        æ ¼å¼è¦æ±‚ï¼š
        ["url1", "url2", "url3"]
        å¦‚æœæ²¡æœ‰ä»»ä½•é“¾æ¥ç›¸å…³ï¼Œè¿”å›ç©ºåˆ—è¡¨: []
        """
        prompt = f"è¯·ä»ä»¥ä¸‹é“¾æ¥ä¸­ç­›é€‰å‡ºæœ€ç›¸å…³çš„æœ€å¤š {MAX_SELECTED_LINKS} ä¸ªï¼š\n\n{link_descriptions}"
        response_text = await self._call_llm(provider, prompt, system_prompt)
        if not response_text:
            return []
        try:
            selected_urls = json.loads(response_text)
            if not isinstance(selected_urls, list):
                raise TypeError("LLM did not return a list")
            final_list = [
                str(url) for url in selected_urls if str(url) in unique_links_dict
            ][:MAX_SELECTED_LINKS]  # ä½¿ç”¨æ›´æ–°åçš„ MAX_SELECTED_LINKS
            logger.info(f"é˜¶æ®µäºŒï¼šLLM ç­›é€‰å®Œæˆï¼Œé€‰å®š {len(final_list)} ä¸ªé“¾æ¥ã€‚")
            return final_list
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(
                f"é˜¶æ®µäºŒï¼šLLM é“¾æ¥ç­›é€‰ç»“æœ JSON è§£æå¤±è´¥ ({e}): {response_text[:200]}..."
            )
            return list(unique_links_dict.keys())[:MAX_SELECTED_LINKS]

    # ------------------ é˜¶æ®µä¸‰ï¼šå†…å®¹å¤„ç†ä¸åˆ†æ (Content Processing & Analysis) ------------------

    async def _resolve_baidu_redirect(self, url: str) -> Optional[str]:
        """è§£æç™¾åº¦é‡å®šå‘é“¾æ¥ï¼Œè·å–çœŸå®URL"""
        try:
            # ç›´æ¥è®¿é—®ç™¾åº¦é‡å®šå‘é“¾æ¥ï¼Œè®©å…¶è‡ªåŠ¨è·³è½¬
            response = await self.client.get(url, follow_redirects=True)
            final_url = str(response.url)

            # å¦‚æœæœ€ç»ˆURLè¿˜æ˜¯ç™¾åº¦åŸŸåï¼Œå¯èƒ½æ˜¯è§£æå¤±è´¥
            if "baidu.com" in final_url:
                logger.warning(f"ç™¾åº¦é‡å®šå‘è§£æå¤±è´¥ï¼Œä»ä¸ºç™¾åº¦åŸŸå: {final_url}")
                return None

            logger.info(f"ç™¾åº¦é‡å®šå‘è§£ææˆåŠŸ: {url} -> {final_url}")
            return final_url

        except Exception as e:
            logger.warning(f"ç™¾åº¦é‡å®šå‘è§£æå¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

    async def _fetch_and_parse_content(self, url: str) -> Optional[str]:
        """
        æŠ“å–å•ä¸ª URL çš„å†…å®¹ï¼Œè§£æå¹¶æ¸…ç† HTMLï¼Œè½¬æ¢ä¸ºçº¯æ–‡æœ¬ã€‚
        ä½¿ç”¨é•¿æœŸå­˜åœ¨çš„ self.client å®ä¾‹ã€‚
        """
        logger.info(f"é˜¶æ®µä¸‰ï¼šæ­£åœ¨æŠ“å– URL: {url} ")

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç™¾åº¦é‡å®šå‘é“¾æ¥å¹¶å°è¯•è§£æ
        if "baidu.com/link" in url:
            logger.info(f"æ£€æµ‹åˆ°ç™¾åº¦é‡å®šå‘é“¾æ¥ï¼Œå°è¯•è§£æ: {url}")
            real_url = await self._resolve_baidu_redirect(url)
            if real_url:
                url = real_url  # ä½¿ç”¨è§£æåçš„çœŸå®URL
                logger.info(f"ä½¿ç”¨è§£æåçš„çœŸå®URL: {url}")
            else:
                logger.info(f"ç™¾åº¦é‡å®šå‘è§£æå¤±è´¥ï¼Œè·³è¿‡å¤„ç†: {url}")
                return None

        html_content = ""
        try:
            # ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨httpxå®¢æˆ·ç«¯
            response = await self.client.get(url)
            response.raise_for_status()  # è§¦å‘ HTTPStatusError

            # è·å–å†…å®¹å¹¶é™åˆ¶å¤§å°
            content = response.content
            if len(content) > MAX_CONTENT_LENGTH * 3:
                logger.warning(f"URL {url} å†…å®¹è¿‡å¤§ï¼Œæˆªæ–­è¯»å–ã€‚")
                content = content[: MAX_CONTENT_LENGTH * 3]

            # è·å–æ–‡æœ¬å†…å®¹
            html_content = response.text
            # --- HTML è§£æä¸æ¸…ç† (ä¿æŒåŸæœ‰é€»è¾‘) ---
            if not html_content:
                return None

            soup = BeautifulSoup(html_content, "lxml")
            # ç§»é™¤ script å’Œ style
            for script in soup(
                ["script", "style", "noscript", "nav", "footer", "header", "aside"]
            ):
                script.decompose()

            # ä¼˜å…ˆå°è¯•è·å– article æ ‡ç­¾
            main_content_tag = (
                soup.find("article") or soup.find("main") or soup.body or soup
            )

            # è½¬æ¢ä¸º markdown å†è½¬å› text ä»¥æ›´å¥½æ¸…ç†æ ¼å¼
            md_text = markdown.markdown(main_content_tag.decode_contents())
            text = "".join(BeautifulSoup(md_text, "lxml").findAll(string=True))
            # æ¸…ç†å¤šä½™ç©ºç™½å’Œæ¢è¡Œ
            cleaned_text = re.sub(r"\s+", " ", text).strip()
            final_text = cleaned_text[:MAX_CONTENT_LENGTH]
            logger.debug(
                f"é˜¶æ®µä¸‰ï¼šURL {url} å†…å®¹æŠ“å–å¹¶æ¸…ç†å®Œæˆï¼Œé•¿åº¦: {len(final_text)}"
            )
            return final_text
            # --- ç»“æŸ HTML è§£æ ---
        # --- ä¿®æ”¹: æ•è·å…·ä½“ httpx å¼‚å¸¸ ---
        except httpx.TimeoutException as e:
            logger.warning(f"æŠ“å– URL {url} è¶…æ—¶ ({FETCH_TIMEOUT}s): {e}")
            return None
        except httpx.HTTPStatusError as e:
            # ç”± raise_for_status() è§¦å‘ï¼Œå¦‚ 404, 500
            logger.warning(
                f"æŠ“å– URL {url} å‘ç”Ÿ HTTP é”™è¯¯: çŠ¶æ€ç ={e.response.status_code}, é”™è¯¯={e}"
            )
            return None
        except httpx.RequestError as e:
            # åŒ…æ‹¬è¿æ¥é”™è¯¯, DNS é”™è¯¯ç­‰
            logger.warning(f"æŠ“å– URL {url} å‘ç”Ÿè¯·æ±‚é”™è¯¯: {e}")
            return None
        # ------------------------------------
        except Exception as e:
            # æ•è· BeautifulSoup, markdown, re ç­‰è§£æè¿‡ç¨‹ä¸­çš„å…¶ä»–é”™è¯¯
            logger.error(
                f"æŠ“å–æˆ–è§£æ URL {url} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True
            )  # ä¿ç•™ exc_info
            return None

    async def _summarize_content(
        self, provider: Provider, query: str, url: str, content: str
    ) -> Optional[str]:
        """ä½¿ç”¨ LLM æ€»ç»“å•ä¸ªæ–‡æ¡£å†…å®¹"""
        logger.info(f"é˜¶æ®µä¸‰ï¼šæ­£åœ¨æ€»ç»“ URL {url} çš„å†…å®¹...")
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œæ€»ç»“å‡ºä¸åŸå§‹æŸ¥è¯¢ï¼šâ€œ{query}â€ é«˜åº¦ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚
        æ€»ç»“åº”æ¸…æ™°ã€ç®€æ´ï¼Œçªå‡ºè¦ç‚¹ã€‚å¿½ç•¥å¹¿å‘Šã€å¯¼èˆªç­‰æ— å…³å†…å®¹ã€‚
        è¯·ç›´æ¥è¿”å›æ€»ç»“æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€æ ‡é¢˜æˆ–é—®å€™è¯­ã€‚
        """
        prompt = f"è¯·æ ¹æ®æŸ¥è¯¢ â€œ{query}â€ æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼š\n\n---\n{content}\n---"
        summary = await self._call_llm(provider, prompt, system_prompt)
        if summary:
            logger.info(f"é˜¶æ®µä¸‰ï¼šURL {url} æ€»ç»“å®Œæˆã€‚")
        else:
            logger.warning(f"é˜¶æ®µä¸‰ï¼šURL {url} æ€»ç»“å¤±è´¥ã€‚")
        return summary

    async def _process_one_link(
        self, provider: Provider, query: str, url: str
    ) -> Optional[Dict[str, str]]:
        """å¤„ç†å•ä¸ªé“¾æ¥ï¼šæŠ“å– -> æ€»ç»“"""
        content = await self._fetch_and_parse_content(url)
        if content and len(content) > 100:  # å¿½ç•¥å†…å®¹è¿‡å°‘çš„é¡µé¢
            summary = await self._summarize_content(provider, query, url, content)
            if summary:
                return {"url": url, "summary": summary}
        return None

    async def _stage3_content_processing(
        self, provider: Provider, query: str, selected_links: List[str]
    ) -> List[Dict[str, str]]:
        """é˜¶æ®µä¸‰ï¼šå¹¶è¡ŒæŠ“å–å†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦"""
        logger.info("é˜¶æ®µä¸‰ï¼šå¼€å§‹å¹¶è¡ŒæŠ“å–å’Œæ€»ç»“å†…å®¹...")
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        tasks = [
            self._process_one_link(provider, query, link) for link in selected_links
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # è¿‡æ»¤æ‰å¤±è´¥æˆ–æ— æ•ˆçš„ç»“æœ
        summaries = [
            res
            for res in results
            if isinstance(res, dict) and res is not None and "summary" in res
        ]
        logger.info(
            f"é˜¶æ®µä¸‰ï¼šæˆåŠŸå¤„ç†å¹¶æ€»ç»“äº† {len(summaries)} / {len(selected_links)} ä¸ªé“¾æ¥ã€‚"
        )
        return summaries

    async def _stage3_aggregation(
        self,
        provider: Provider,
        query: str,
        expansion_questions: List[str],
        summaries: List[Dict[str, str]],
    ) -> Optional[str]:
        """é˜¶æ®µä¸‰ï¼šLLM èšåˆåˆ†ææ‰€æœ‰æ‘˜è¦ï¼Œç”Ÿæˆ Markdown æŠ¥å‘Š"""
        logger.info("é˜¶æ®µä¸‰ï¼šå¼€å§‹èšåˆåˆ†ææ‰€æœ‰æ‘˜è¦...")
        if not summaries:
            return "æœªèƒ½ä»ä»»ä½•æ¥æºè·å–æœ‰æ•ˆæ‘˜è¦ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚"
        # å‡†å¤‡ LLM è¾“å…¥
        summaries_input = "\n\n".join(
            [f"### æ¥æº: {item['url']}\n{item['summary']}\n---" for item in summaries]
        )
        expansion_q_str = (
            "\n".join([f"- {q}" for q in expansion_questions])
            if expansion_questions
            else "æ— "
        )
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªé«˜çº§ç ”ç©¶åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ç»¼åˆæ¥è‡ªå¤šä¸ªæ¥æºçš„æ‘˜è¦ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°ã€å†…å®¹è¿è´¯ã€é€»è¾‘ä¸¥å¯†çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚

        åŸå§‹æŸ¥è¯¢: "{query}"

        éœ€è¦é¢å¤–è€ƒè™‘å’Œå›ç­”çš„æ‰©å±•é—®é¢˜:
        {expansion_q_str}
        æŠ¥å‘Šè¦æ±‚ï¼š
        1. æ ¼å¼ï¼šä½¿ç”¨æ ‡å‡†çš„ Markdown è¯­æ³•ã€‚
        2. ç»“æ„ï¼šåº”åŒ…å«æ ‡é¢˜ã€å¼•è¨€ã€ä¸»ä½“æ®µè½ï¼ˆå¯ä»¥æŒ‰ä¸»é¢˜æˆ–æ‰©å±•é—®é¢˜åˆ†èŠ‚ï¼‰ã€ç»“è®ºã€‚
        3. å†…å®¹ï¼šç»¼åˆæ‰€æœ‰æ¥æºçš„ä¿¡æ¯ï¼Œå¯¹æ¯”ä¸åŒè§‚ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œæ•´åˆä¿¡æ¯ï¼Œæ„å»ºé€»è¾‘ã€‚
        4. å¼•ç”¨ï¼šåœ¨å¼•ç”¨äº†æŸä¸ªæ¥æºä¿¡æ¯çš„å¥å­æˆ–æ®µè½æœ«å°¾ï¼Œæ˜ç¡®æ ‡æ³¨æ¥æºï¼Œæ ¼å¼ä¸º ` [æ¥æº: URL]`ã€‚
        5. ç›®æ ‡ï¼šå…¨é¢ã€æ·±å…¥åœ°å›ç­”åŸå§‹æŸ¥è¯¢åŠæ‰©å±•é—®é¢˜ã€‚
        6. è¾“å‡ºï¼šç›´æ¥è¾“å‡º Markdown æŠ¥å‘Šæ­£æ–‡ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–é—®å€™è¯­ã€‚
        """
        prompt = f"è¯·æ ¹æ®ä»¥ä¸‹æ¥è‡ªä¸åŒæ¥æºçš„æ‘˜è¦ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½å…³äº â€œ{query}â€ çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼š\n\n{summaries_input}"
        report_markdown = await self._call_llm(provider, prompt, system_prompt)
        if report_markdown:
            logger.info("é˜¶æ®µä¸‰ï¼šèšåˆåˆ†æå®Œæˆï¼ŒMarkdown æŠ¥å‘Šå·²ç”Ÿæˆã€‚")
        else:
            logger.warning("é˜¶æ®µä¸‰ï¼šèšåˆåˆ†æå¤±è´¥ã€‚")
        return report_markdown

    # ------------------ é˜¶æ®µå››ï¼šæŠ¥å‘Šç”Ÿæˆä¸äº¤ä»˜ (Report Generation & Delivery) ------------------
    async def _stage4_report_generation(
        self, markdown_text: str, output_format: str = None
    ) -> Optional[Any]:
        """é˜¶æ®µå››ï¼šä½¿ç”¨è¾“å‡ºæ ¼å¼ç®¡ç†å™¨ç”ŸæˆæŠ¥å‘Š"""
        if not output_format:
            output_format = self.output_manager.get_default_format()

        logger.info(f"é˜¶æ®µå››ï¼šå¼€å§‹ç”Ÿæˆ {output_format} æ ¼å¼æŠ¥å‘Š...")

        try:
            # ä½¿ç”¨è¾“å‡ºæ ¼å¼ç®¡ç†å™¨æ ¼å¼åŒ–æŠ¥å‘Š
            result = await self.output_manager.format_report(
                markdown_content=markdown_text,
                format_name=output_format,
                star_instance=self,  # ä¼ é€’Starå®ä¾‹ç”¨äºå›¾ç‰‡æ¸²æŸ“
            )

            if result:
                logger.info(f"é˜¶æ®µå››ï¼š{output_format} æ ¼å¼æŠ¥å‘Šç”ŸæˆæˆåŠŸã€‚")
            else:
                logger.warning(f"é˜¶æ®µå››ï¼š{output_format} æ ¼å¼æŠ¥å‘Šç”Ÿæˆå¤±è´¥ã€‚")

            return result
        except Exception as e:
            logger.error(
                f"é˜¶æ®µå››ï¼šç”Ÿæˆ {output_format} æ ¼å¼æŠ¥å‘Šå¤±è´¥: {e}", exc_info=True
            )
            return None

    # ------------------ ä¸»æµç¨‹æ§åˆ¶ ------------------

    async def _run_research_pipeline(
        self, event: AstrMessageEvent, query: str, output_format: str = None
    ) -> AsyncGenerator[MessageEventResult, None]:
        """æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶æµç¨‹ç®¡çº¿ï¼Œä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨å‘é€ä¸­é—´çŠ¶æ€å’Œæœ€ç»ˆç»“æœ"""
        # æ£€æŸ¥ LLM
        provider = self.context.get_using_provider()
        if not provider:
            yield event.plain_result(
                "âŒ é”™è¯¯ï¼šæœªé…ç½®æˆ–å¯ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œæ— æ³•æ‰§è¡Œç ”ç©¶ã€‚"
            )
            return

        start_time = asyncio.get_running_loop().time()
        yield event.plain_result(
            f"ğŸ” æ”¶åˆ°ç ”ç©¶è¯·æ±‚: '{query}'\nâ³ å¼€å§‹é˜¶æ®µä¸€ï¼šæŸ¥è¯¢å¤„ç†ä¸æ‰©å±•..."
        )

        try:
            # é˜¶æ®µä¸€
            parsed_query = await self._stage1_query_processing(provider, query)
            if not parsed_query or not parsed_query.get("all_search_terms"):
                yield event.plain_result("âŒ é˜¶æ®µä¸€å¤±è´¥ï¼šLLMæœªèƒ½æœ‰æ•ˆè§£ææŸ¥è¯¢ã€‚")
                return
            yield event.plain_result(
                "âœ… é˜¶æ®µä¸€å®Œæˆã€‚\nâ³ å¼€å§‹é˜¶æ®µäºŒï¼šä¿¡æ¯æ£€ç´¢ä¸ç­›é€‰..."
            )
            # é˜¶æ®µäºŒ
            search_terms = parsed_query.get("search_queries", []) or parsed_query.get(
                "all_search_terms", []
            )
            initial_links = await self._search_web(search_terms)
            if not initial_links:
                yield event.plain_result(
                    "âš ï¸ é˜¶æ®µäºŒè­¦å‘Šï¼šç½‘ç»œæœç´¢æœªè¿”å›ä»»ä½•åˆå§‹ç»“æœï¼ˆæˆ–æœç´¢åŠŸèƒ½æœªå®ç°ï¼‰ã€‚"
                )
                # å¦‚æœæœç´¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®©LLMå›ç­”
                yield event.plain_result("âš ï¸ å°è¯•è®©LLMæ ¹æ®è‡ªèº«çŸ¥è¯†ç›´æ¥ç”ŸæˆæŠ¥å‘Š...")
                direct_summary = await self._summarize_content(
                    provider,
                    query,
                    "LLM Knowledge Base",
                    "è¯·åŸºäºä½ è‡ªèº«çš„çŸ¥è¯†åº“ï¼Œç”Ÿæˆä¸€ä»½å…³äºæ­¤ä¸»é¢˜çš„æŠ¥å‘Šã€‚",
                )
                if direct_summary:
                    summaries = [
                        {"url": "LLM Knowledge Base", "summary": direct_summary}
                    ]
                    selected_links = ["LLM Knowledge Base"]
                else:
                    yield event.plain_result(
                        "âŒ é˜¶æ®µäºŒå¤±è´¥ï¼šæœç´¢å’ŒLLMè‡ªèº«çŸ¥è¯†å‡æ— æ³•æä¾›ä¿¡æ¯ã€‚"
                    )
                    return
            else:
                yield event.plain_result(
                    f"â„¹ï¸ æœç´¢åˆ° {len(initial_links)} ä¸ªåˆå§‹é“¾æ¥ï¼Œå¼€å§‹ç­›é€‰..."
                )
                selected_links = await self._stage2_link_selection(
                    provider, query, initial_links
                )
                if not selected_links:
                    yield event.plain_result(
                        "âŒ é˜¶æ®µäºŒå¤±è´¥ï¼šLLMæœªèƒ½ä»ç»“æœä¸­ç­›é€‰å‡ºç›¸å…³é“¾æ¥ã€‚"
                    )
                    return
                yield event.plain_result(
                    f"âœ… é˜¶æ®µäºŒå®Œæˆã€‚ç­›é€‰å‡º {len(selected_links)} ä¸ªé“¾æ¥ã€‚\nâ³ å¼€å§‹é˜¶æ®µä¸‰ï¼šå†…å®¹å¤„ç†ä¸åˆ†æ..."
                )
                # é˜¶æ®µä¸‰ - å¤„ç†
                summaries = await self._stage3_content_processing(
                    provider, query, selected_links
                )
                if not summaries:
                    yield event.plain_result(
                        "âŒ é˜¶æ®µä¸‰å¤±è´¥ï¼šæœªèƒ½ä»ä»»ä½•é€‰å®šé“¾æ¥æŠ“å–æˆ–æ€»ç»“æœ‰æ•ˆå†…å®¹ã€‚"
                    )
                    return
                yield event.plain_result(
                    f"â„¹ï¸ å·²æŠ“å–å¹¶æ€»ç»“ {len(summaries)} ç¯‡å†…å®¹ã€‚å¼€å§‹èšåˆåˆ†æ..."
                )

            # é˜¶æ®µä¸‰ - èšåˆ
            aggregated_markdown = await self._stage3_aggregation(
                provider, query, parsed_query.get("expansion_questions", []), summaries
            )
            if not aggregated_markdown:
                yield event.plain_result("âŒ é˜¶æ®µä¸‰å¤±è´¥ï¼šLLMå†…å®¹èšåˆåˆ†æå¤±è´¥ã€‚")
                return
            yield event.plain_result(
                "âœ… é˜¶æ®µä¸‰å®Œæˆã€‚\nâ³ å¼€å§‹é˜¶æ®µå››ï¼šæŠ¥å‘Šç”Ÿæˆä¸æ¸²æŸ“..."
            )
            # é˜¶æ®µå››
            report_result = await self._stage4_report_generation(
                aggregated_markdown, output_format
            )

            end_time = asyncio.get_running_loop().time()
            duration = round(end_time - start_time, 2)

            # è·å–å®é™…ä½¿ç”¨çš„è¾“å‡ºæ ¼å¼
            actual_format = output_format or self.output_manager.get_default_format()

            # æœ€ç»ˆè¾“å‡º
            status_msg = f"âœ… æ·±åº¦ç ”ç©¶å®Œæˆï¼æ€»è€—æ—¶: {duration} ç§’ã€‚"

            if report_result:
                if actual_format == "image":
                    # å›¾ç‰‡æ ¼å¼ï¼šä½¿ç”¨æ¶ˆæ¯é“¾å‘é€æ–‡æœ¬å’Œå›¾ç‰‡
                    yield event.chain_result(
                        [
                            Comp.Plain(text=status_msg + "\nä¸ºæ‚¨ç”Ÿæˆäº†å›¾ç‰‡æŠ¥å‘Šï¼š"),
                            Comp.Image.fromURL(report_result),
                        ]
                    )
                elif actual_format == "html":
                    # HTMLæ ¼å¼ï¼šä½¿ç”¨Fileç»„ä»¶å‘é€HTMLæ–‡ä»¶
                    import os

                    filename = os.path.basename(report_result)
                    yield event.chain_result(
                        [
                            Comp.Plain(
                                text=status_msg + "\nä¸ºæ‚¨ç”Ÿæˆäº†HTMLæŠ¥å‘Šï¼š"
                            ),
                            Comp.File(name=filename, file=report_result),
                        ]
                    )
                else:
                    # å…¶ä»–æ ¼å¼ï¼šç›´æ¥è¿”å›ç»“æœ
                    yield event.plain_result(
                        status_msg
                        + f"\nä¸ºæ‚¨ç”Ÿæˆäº†{actual_format}æ ¼å¼æŠ¥å‘Šï¼š\n\n{report_result}"
                    )
            else:
                # æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹Markdown
                yield event.plain_result(
                    status_msg
                    + f"\nâš ï¸ {actual_format}æ ¼å¼æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä»¥ä¸‹ä¸ºåŸå§‹ Markdown æŠ¥å‘Šï¼š\n---\n"
                    + aggregated_markdown
                )
        except asyncio.TimeoutError:
            yield event.plain_result("âŒ ç ”ç©¶è¿‡ç¨‹è¶…æ—¶ã€‚")
            logger.error("Pipeline Timeout", exc_info=True)
        except Exception as e:
            yield event.plain_result(
                f"âŒ ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {type(e).__name__} - {e}"
            )
            logger.error(f"Pipeline error for query '{query}': {e}", exc_info=True)

    @filter.command("deepresearch", alias={"ç ”ç©¶", "æ·±åº¦ç ”ç©¶"})
    async def handle_research_command(
        self, event: AstrMessageEvent, query: str = "", output_format: str = "image"
    ):
        """
        æŒ‡ä»¤: /deepresearch <æŸ¥è¯¢å†…å®¹> [è¾“å‡ºæ ¼å¼]
        å¯¹æŒ‡å®šå†…å®¹è¿›è¡Œå¤šé˜¶æ®µæ·±åº¦ç ”ç©¶å¹¶ç”ŸæˆæŠ¥å‘Šã€‚
        """
        if not query:
            available_formats = self.output_manager.get_available_formats()
            formats_text = "\n".join(
                [
                    f"  - {fmt['name']}: {fmt['description']}"
                    for fmt in available_formats
                ]
            )

            yield event.plain_result(
                f"è¯·è¾“å…¥è¦ç ”ç©¶çš„å†…å®¹ã€‚\n\n"
                f"ç”¨æ³•: /deepresearch <æŸ¥è¯¢å†…å®¹> [è¾“å‡ºæ ¼å¼]\n\n"
                f"æ”¯æŒçš„è¾“å‡ºæ ¼å¼:\n{formats_text}\n\n"
                f"ç¤ºä¾‹:\n"
                f"  /deepresearch äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿\n"
                f"  /deepresearch Pythonç¼–ç¨‹ markdown\n"
                f"  /deepresearch åŒºå—é“¾æŠ€æœ¯ html"
            )
            return

        # éªŒè¯è¾“å‡ºæ ¼å¼æ˜¯å¦æ”¯æŒ
        if not self.output_manager.is_format_supported(output_format):
            available_formats = [
                fmt["name"] for fmt in self.output_manager.get_available_formats()
            ]
            yield event.plain_result(
                f"âŒ ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: '{output_format}'\n"
                f"æ”¯æŒçš„æ ¼å¼: {', '.join(available_formats)}"
            )
            return

        logger.info(f"ç”¨æˆ·æŒ‡å®šè¾“å‡ºæ ¼å¼: {output_format}")

        # ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨æ¨¡å¼ï¼Œé€ä¸ª yield æ¶ˆæ¯
        async for message_result in self._run_research_pipeline(
            event, query, output_format
        ):
            yield message_result
        event.stop_event()  # åœæ­¢äº‹ä»¶ä¼ æ’­ï¼Œé˜²æ­¢LLMå†æ¬¡é»˜è®¤å›å¤
